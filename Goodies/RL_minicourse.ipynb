{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sitraka17/Learning-Data-Science/blob/main/RL_minicourse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup requirements and import dependencies"
      ],
      "metadata": {
        "id": "HyV72TCzYwJd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX8InBuvYoNG"
      },
      "outputs": [],
      "source": [
        "!pip install torch ray[rllib]==1.12.1 gym Box2D matplotlib tensorboard redis gym-super-mario-bros pickle5\n",
        "!test -e utils.py && rm utils.py\n",
        "!wget \"https://matthieu-zimmer.net/~matthieu/courses/rl-minicourse/colab/utils.py\"\n",
        "\n",
        "import torch # NN framework\n",
        "import gym  # RL environment\n",
        "import ray  # RL algorithms\n",
        "import time\n",
        "import utils\n",
        "from ray.rllib.agents.ppo import PPOTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Interaction Loop: Exercise 1\n",
        "\n",
        "Interact with an environment for one complete trajectory by playing random actions and print the sum of reward obtained"
      ],
      "metadata": {
        "id": "SrJKbHXaZcFj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HedOS6sUZglx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Interaction Loop: Exercise 2\n",
        "With the help of print and the display of the environment, guess if the actions are continuous or discrete (how many possible action if discrete), what the meaning of the action is, what the state is and what the reward function is."
      ],
      "metadata": {
        "id": "8tMvZ0IcZg5y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M9gO66HuZlOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training\n",
        "Train a policy with the PPO algorithm for 10 iterations."
      ],
      "metadata": {
        "id": "T7X0F_XLaMZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QeX0ZiyqaSy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Monitoring Training "
      ],
      "metadata": {
        "id": "aSjTUr6UfafI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results"
      ],
      "metadata": {
        "id": "XNxgh1Ldfovw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Saving and Loading a policy"
      ],
      "metadata": {
        "id": "-U1gh9NseMsL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b32iKNQufpTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Evaluation: Exercise 3\n",
        "\n",
        "Load and evaluate the policy you trained before. To do so, you can reuse code from Exercise 1, but instead of playing random action, use the new policy.\n",
        "Instead of evaluating the policy on one trajectory, use an average over multiples trajectories."
      ],
      "metadata": {
        "id": "3PP9RGvcxTVf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "puA4cRs4z4vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Hyperparameters Optimisation: Exercise 4\n",
        "\n",
        "Try obtaining higher rewards by retraining with other hyperparameters. "
      ],
      "metadata": {
        "id": "jFdOrDPC_wua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# default values of some hyperparemeters of PPO\n",
        "default_config = {\n",
        "    \"framework\": \"torch\",\n",
        "    \"num_gpus\": 0,\n",
        "    \"num_workers\": 1,\n",
        "    \"num_cpus_per_worker\": 1,\n",
        "    \"num_envs_per_worker\": 1,\n",
        "    \"gamma\": 0.99,\n",
        "    \"lr\": 0.0001,\n",
        "    \"train_batch_size\": 200,\n",
        "}"
      ],
      "metadata": {
        "id": "2Z18Di7L_5Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Customised Environment: Exercise 5\n",
        "By looking at the simple Shower environment (defined below), create your own new environment to reproduce the following MDP: https://en.wikipedia.org/wiki/File:Markov_Decision_Process.svg\n"
      ],
      "metadata": {
        "id": "Utkh3sRQ038E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from ray.tune import register_env\n",
        "\n",
        "class ShowerEnv(gym.Env):\n",
        "  def __init__(self):\n",
        "    # 3 discrete actions\n",
        "    self.action_space = gym.spaces.Discrete(3)\n",
        "    # single continuous scalar to describe the state\n",
        "    self.observation_space = gym.spaces.Box(low=0, high=75, shape=(1,))\n",
        "\n",
        "  def reset(self):\n",
        "    # initial temperature between 33° and 43°\n",
        "    self.temperature = np.array([38 + np.random.randint(-5, 5)]).astype(float)\n",
        "    # number of step in one shower\n",
        "    self.shower_length = 10\n",
        "  \n",
        "  def step(self, action):\n",
        "    # the new temperature depends on the action and some random values\n",
        "    self.temperature += np.random.randint(-3, 2) + action\n",
        "\n",
        "    # decrease length\n",
        "    self.shower_length -= 1\n",
        "\n",
        "    # stop the episode once the length is 0\n",
        "    done = self.shower_length <= 0\n",
        "\n",
        "    # compute reward\n",
        "    if self.temperature >= 37 and self.temperature <= 39:\n",
        "      reward = 1\n",
        "    else:\n",
        "      reward = -1\n",
        "\n",
        "    return self.temperature, reward, done, []\n",
        "\n",
        "# register the environment with rllib\n",
        "register_env('MyShowerEnv-v0', lambda x: ShowerEnv())\n",
        "# instead of using gym.make use the following\n",
        "env = ShowerEnv()\n",
        "\n",
        "env.reset()\n",
        "env.step(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvLRzIvi1BQg",
        "outputId": "bbf4de2a-e488-41ef-cee6-93885deb2064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([37.]), 1, False, [])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Try other algorithms: Exercise 6\n",
        "Rllib features numerous algorithms. Visit https://docs.ray.io/en/latest/rllib/rllib-algorithms.html, pick one that suits your needs and use it to train an agent on your customis\n",
        "ed environment. "
      ],
      "metadata": {
        "id": "0LWnTrPl1B6N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iUUUzQPD1CEb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}